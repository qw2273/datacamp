{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 Understand  the problem  \n",
    "## data type:\n",
    "- tabular data\n",
    "- time series\n",
    "- image/text data\n",
    "    \n",
    "## problem type\n",
    "- classification\n",
    "- regression \n",
    "- ranking\n",
    "\n",
    "## evaluation metric\n",
    "- roc auc\n",
    "- f1 \n",
    "- mse\n",
    "- mae\n",
    "- log loss for classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2  EDA\n",
    "Goals\n",
    "- size of  data\n",
    "- properties of target variable : unblanced , skewed \n",
    "- properties of features: look for peculiarities and dependencies between features and target variable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 Feature Engineer\n",
    "## Encode\n",
    "### label encode: \n",
    "    - assign different number(0, 1, 2, ... ) for different category, ranking dependency \n",
    "    - this is okay for tree-based model, but harmful for linear models\n",
    "### onhot encode: \n",
    "    - 1 for category and o for others\n",
    "    - pd.get_dummies(df[''] ,prefix = '')\n",
    "     drop original featue \n",
    "### target encode\n",
    "\n",
    "    - mean target encoding\n",
    "        1. on train,  calculate mean of target by category -> apply to test \n",
    "        2. split train into K folds, calculate mean on (K-1) folds , apply to Kth fold\n",
    "        3. apply mean target encoded feature to the model\n",
    "    - practical guide: rare category with only one or two category, will get strict 0 or 1 mean encoding\n",
    "        1. smoothing\n",
    "            smoothed_mean_enc of category i = (target_sum of category i  + alpha * global mean) / size of category i + alpha \n",
    "            alpha usually between 5 to 10 \n",
    "        2. new category\n",
    "            fill new category with global meanFor binary classification usually mean target encoding is used\n",
    " \n",
    "#### target encoding tye: \n",
    "- For _binary classification_ usually __mean target encoding__ is used\n",
    "- For _regression_ __mean could be changed to median, quartiles, etc__.\n",
    "- For _multi-class classification_ with N classes we create __N features with target mean for each category in one vs. all fashion__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data Input\n",
    "### Numerical data\n",
    "- mean/median imputation\n",
    "- constant value imputation\n",
    "    - missing value = -999, this works for tree-based model. but not good for linear model\n",
    "### Categorical data\n",
    "- most frequent category imputation\n",
    "- new category imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 Modeling\n",
    "### Hyperparameter optimization\n",
    "\n",
    "| model type   | feature engineer   |   hyperparameter tuning     |\n",
    "| :------------- | :----------: | -----------: |\n",
    "| classic ML |  +++ | +|\n",
    "|deep learning | no need to do | +++|\n",
    "\n",
    "### Model ensembling\n",
    "#### 1. Model blending: average of multiple models' predictions\n",
    "#### 2. Model stacking:  The idea is to train multiple single models, take their predictions and use these predictions as features in the 2nd level model.\n",
    "    So, we need to perform the following steps: \n",
    "        1. Split train data into two parts\n",
    "        2. Train multiple models on Part 1\n",
    "        3. Make predictions on Part 2\n",
    "        4. Make predictions on the test data\n",
    "        5. Train a new model on Part 2 using predictions as features. This model is called the __2nd level model or meta-model__. \n",
    "        6. Make predictions on the test data using the 2nd level model. its called stacking output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE OF MEAN TARGET SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mean_target_encoding(train, test, target, categorical, alpha=5):\n",
    "    # Calculate global mean on the train data\n",
    "    global_mean = train[target].mean()\n",
    "    \n",
    "    # Group by the categorical feature and calculate its properties\n",
    "    train_groups = train.groupby(categorical)\n",
    "    category_sum = train_groups[target].sum()\n",
    "    category_size = train_groups.size()\n",
    "    \n",
    "    # Calculate smoothed mean target statistics\n",
    "    train_statistics = (category_sum + global_mean * alpha) / (category_size + alpha)\n",
    "    \n",
    "    # Apply statistics to the test data and fill new categories\n",
    "    test_feature = test[categorical].map(train_statistics).fillna(global_mean)\n",
    "    print(train_statistics)\n",
    "    return test_feature.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mean_target_encoding(train, target, categorical, kf_object ,  alpha=5):\n",
    "\n",
    "    train_feature = pd.Series(index=train.index)\n",
    "    \n",
    "    # For each folds split\n",
    "    for train_index, valid_index in kf_object :\n",
    "        cv_train, cv_valid = train.iloc[train_index], train.iloc[valid_index]\n",
    "      \n",
    "        # Calculate out-of-fold statistics and apply to cv_test\n",
    "        cv_valid_feature = test_mean_target_encoding(cv_train, cv_valid, target, categorical, alpha)\n",
    "        \n",
    "        # Save new feature for this particular fold\n",
    "        train_feature.iloc[valid_index] = cv_valid_feature       \n",
    "    return train_feature.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_target_encoding(train, test, target, categorical, kf_object,  alpha=5):\n",
    "  \n",
    "    # Get the cv train feature\n",
    "    train_feature = train_mean_target_encoding(train, target, categorical, kf_object, alpha)\n",
    "    \n",
    "    # Get the test feature\n",
    "    test_feature = test_mean_target_encoding(train, test, target, categorical, alpha)\n",
    "    \n",
    "    # Return new features to add to the model\n",
    "    return train_feature, test_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interest_level\n",
      "high      2700.972007\n",
      "low       4002.263539\n",
      "medium    3152.018955\n",
      "dtype: float64\n",
      "interest_level\n",
      "high      2687.557962\n",
      "low       4302.890160\n",
      "medium    3161.397332\n",
      "dtype: float64\n",
      "interest_level\n",
      "high      2725.533743\n",
      "low       4224.189221\n",
      "medium    3165.572935\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Qiqi/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create a StratifiedKFold object\n",
    "str_kf = StratifiedKFold(n_splits =3, shuffle=True, random_state=123)\n",
    "str_kf_obj = str_kf.split(train_data, train_data['interest_level'])\n",
    "rtn_1 = train_mean_target_encoding(train= train_data, target= 'price', categorical= 'interest_level', kf_object=str_kf_obj, alpha = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE OF MISSING VALUE IMPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "# mean imputatin\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "constant_imputer = SimpleImputer(strategy='constant', fill_value= -999)\n",
    "# mean_imputer.fit_transform(df[[]]) have to use double brackets here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bathrooms          0\n",
       "bedrooms           0\n",
       "description        0\n",
       "display_address    0\n",
       "features           0\n",
       "latitude           0\n",
       "longitude          0\n",
       "price              0\n",
       "street_address     0\n",
       "interest_level     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE OF STACKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "# 1.  Split train data into two parts\n",
    "part_1, part_2 = train_test_split(train, test_size=0.5, random_state=123)\n",
    "\n",
    "# 2. Train a Gradient Boosting model on Part 1\n",
    "gb = GradientBoostingRegressor().fit(part_1[features], part_1.fare_amount)\n",
    "\n",
    "# Train a Random Forest model on Part 1\n",
    "rf = RandomForestRegressor().fit(part_1[features], part_1.fare_amount)\n",
    "\n",
    "# 3. Make predictions on the Part 2 data\n",
    "part_2['gb_pred'] = gb.predict(part_2[features])\n",
    "part_2['rf_pred'] = rf.predict(part_2[features])\n",
    "\n",
    "# 4. Make predictions on the test data\n",
    "test['gb_pred'] = gb.predict(test[features])\n",
    "test['rf_pred'] = rf.predict(test[features])\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 5. Create linear regression model without the intercept\n",
    "lr = LinearRegression(fit_intercept=False)\n",
    "\n",
    "# 6. Train 2nd level model on the Part 2 data\n",
    "lr.fit(part_2[['gb_pred', 'rf_pred']], part_2.fare_amount)\n",
    "\n",
    "# Look at the model coefficients: higher coefficient means more trust\n",
    "print(lr.coef_)\n",
    "\n",
    "# 7. Make stacking predictions on the test data\n",
    "test['stacking'] = lr.predict(test[['gb_pred', 'rf_pred']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
